{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Inverse indexing, index search, and signal page rankÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## PART I: Preparing the documents/webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/dhanendra/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d299aea0d2f1a76a70043d71b9233b04d985147d"
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import string\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14826',\n",
       " 'test/14828',\n",
       " 'test/14829',\n",
       " 'test/14832',\n",
       " 'test/14833',\n",
       " 'test/14839',\n",
       " 'test/14840',\n",
       " 'test/14841',\n",
       " 'test/14842',\n",
       " 'test/14843',\n",
       " 'test/14844',\n",
       " 'test/14849',\n",
       " 'test/14852',\n",
       " 'test/14854',\n",
       " 'test/14858',\n",
       " 'test/14859',\n",
       " 'test/14860',\n",
       " 'test/14861',\n",
       " 'test/14862',\n",
       " 'test/14863',\n",
       " 'test/14865',\n",
       " 'test/14867',\n",
       " 'test/14872',\n",
       " 'test/14873',\n",
       " 'test/14875',\n",
       " 'test/14876',\n",
       " 'test/14877',\n",
       " 'test/14881',\n",
       " 'test/14882',\n",
       " 'test/14885',\n",
       " 'test/14886',\n",
       " 'test/14888',\n",
       " 'test/14890',\n",
       " 'test/14891',\n",
       " 'test/14892',\n",
       " 'test/14899',\n",
       " 'test/14900',\n",
       " 'test/14903',\n",
       " 'test/14904',\n",
       " 'test/14907',\n",
       " 'test/14909',\n",
       " 'test/14911',\n",
       " 'test/14912',\n",
       " 'test/14913',\n",
       " 'test/14918',\n",
       " 'test/14919',\n",
       " 'test/14921',\n",
       " 'test/14922',\n",
       " 'test/14923',\n",
       " 'test/14926',\n",
       " 'test/14928',\n",
       " 'test/14930',\n",
       " 'test/14931',\n",
       " 'test/14932',\n",
       " 'test/14933',\n",
       " 'test/14934',\n",
       " 'test/14941',\n",
       " 'test/14943',\n",
       " 'test/14949',\n",
       " 'test/14951',\n",
       " 'test/14954',\n",
       " 'test/14957',\n",
       " 'test/14958',\n",
       " 'test/14959',\n",
       " 'test/14960',\n",
       " 'test/14962',\n",
       " 'test/14963',\n",
       " 'test/14964',\n",
       " 'test/14965',\n",
       " 'test/14967',\n",
       " 'test/14968',\n",
       " 'test/14969',\n",
       " 'test/14970',\n",
       " 'test/14971',\n",
       " 'test/14974',\n",
       " 'test/14975',\n",
       " 'test/14978',\n",
       " 'test/14981',\n",
       " 'test/14982',\n",
       " 'test/14983',\n",
       " 'test/14984',\n",
       " 'test/14985',\n",
       " 'test/14986',\n",
       " 'test/14987',\n",
       " 'test/14988',\n",
       " 'test/14993',\n",
       " 'test/14995',\n",
       " 'test/14998',\n",
       " 'test/15000',\n",
       " 'test/15001',\n",
       " 'test/15002',\n",
       " 'test/15004',\n",
       " 'test/15005',\n",
       " 'test/15006',\n",
       " 'test/15011',\n",
       " 'test/15012',\n",
       " 'test/15013',\n",
       " 'test/15016',\n",
       " 'test/15017',\n",
       " 'test/15020',\n",
       " 'test/15023',\n",
       " 'test/15024',\n",
       " 'test/15026',\n",
       " 'test/15027',\n",
       " 'test/15028',\n",
       " 'test/15029',\n",
       " 'test/15031',\n",
       " 'test/15032',\n",
       " 'test/15033',\n",
       " 'test/15037',\n",
       " 'test/15038',\n",
       " 'test/15043',\n",
       " 'test/15045',\n",
       " 'test/15046',\n",
       " 'test/15048',\n",
       " 'test/15049',\n",
       " 'test/15052',\n",
       " 'test/15053',\n",
       " 'test/15055',\n",
       " 'test/15056',\n",
       " 'test/15060',\n",
       " 'test/15061',\n",
       " 'test/15062',\n",
       " 'test/15063',\n",
       " 'test/15065',\n",
       " 'test/15067',\n",
       " 'test/15069',\n",
       " 'test/15070',\n",
       " 'test/15074',\n",
       " 'test/15077',\n",
       " 'test/15078',\n",
       " 'test/15079',\n",
       " 'test/15082',\n",
       " 'test/15090',\n",
       " 'test/15091',\n",
       " 'test/15092',\n",
       " 'test/15093',\n",
       " 'test/15094',\n",
       " 'test/15095',\n",
       " 'test/15096',\n",
       " 'test/15097',\n",
       " 'test/15103',\n",
       " 'test/15104',\n",
       " 'test/15106',\n",
       " 'test/15107',\n",
       " 'test/15109',\n",
       " 'test/15110',\n",
       " 'test/15111',\n",
       " 'test/15112',\n",
       " 'test/15118',\n",
       " 'test/15119',\n",
       " 'test/15120',\n",
       " 'test/15121',\n",
       " 'test/15122',\n",
       " 'test/15124',\n",
       " 'test/15126',\n",
       " 'test/15128',\n",
       " 'test/15129',\n",
       " 'test/15130',\n",
       " 'test/15132',\n",
       " 'test/15136',\n",
       " 'test/15138',\n",
       " 'test/15141',\n",
       " 'test/15144',\n",
       " 'test/15145',\n",
       " 'test/15146',\n",
       " 'test/15149',\n",
       " 'test/15152',\n",
       " 'test/15153',\n",
       " 'test/15154',\n",
       " 'test/15156',\n",
       " 'test/15157',\n",
       " 'test/15161',\n",
       " 'test/15162',\n",
       " 'test/15171',\n",
       " 'test/15172',\n",
       " 'test/15175',\n",
       " 'test/15179',\n",
       " 'test/15180',\n",
       " 'test/15185',\n",
       " 'test/15188',\n",
       " 'test/15189',\n",
       " 'test/15190',\n",
       " 'test/15193',\n",
       " 'test/15194',\n",
       " 'test/15197',\n",
       " 'test/15198',\n",
       " 'test/15200',\n",
       " 'test/15204',\n",
       " 'test/15205',\n",
       " 'test/15206',\n",
       " 'test/15207',\n",
       " 'test/15208',\n",
       " 'test/15210',\n",
       " 'test/15211',\n",
       " 'test/15212',\n",
       " 'test/15213',\n",
       " 'test/15217',\n",
       " 'test/15219',\n",
       " 'test/15220',\n",
       " 'test/15221',\n",
       " 'test/15222',\n",
       " 'test/15223',\n",
       " 'test/15226',\n",
       " 'test/15227',\n",
       " 'test/15230',\n",
       " 'test/15233',\n",
       " 'test/15234',\n",
       " 'test/15237',\n",
       " 'test/15238',\n",
       " 'test/15239',\n",
       " 'test/15240',\n",
       " 'test/15242',\n",
       " 'test/15243',\n",
       " 'test/15244',\n",
       " 'test/15246',\n",
       " 'test/15247',\n",
       " 'test/15250',\n",
       " 'test/15253',\n",
       " 'test/15254',\n",
       " 'test/15255',\n",
       " 'test/15258',\n",
       " 'test/15259',\n",
       " 'test/15262',\n",
       " 'test/15263',\n",
       " 'test/15264',\n",
       " 'test/15265',\n",
       " 'test/15270',\n",
       " 'test/15271',\n",
       " 'test/15273',\n",
       " 'test/15274',\n",
       " 'test/15276',\n",
       " 'test/15278',\n",
       " 'test/15280',\n",
       " 'test/15281',\n",
       " 'test/15283',\n",
       " 'test/15287',\n",
       " 'test/15290',\n",
       " 'test/15292',\n",
       " 'test/15294',\n",
       " 'test/15295',\n",
       " 'test/15296',\n",
       " 'test/15299',\n",
       " 'test/15300',\n",
       " 'test/15302',\n",
       " 'test/15303',\n",
       " 'test/15306',\n",
       " 'test/15307',\n",
       " 'test/15308',\n",
       " 'test/15309',\n",
       " 'test/15310',\n",
       " 'test/15311',\n",
       " 'test/15312',\n",
       " 'test/15313',\n",
       " 'test/15314',\n",
       " 'test/15315',\n",
       " 'test/15321',\n",
       " 'test/15322',\n",
       " 'test/15324',\n",
       " 'test/15325',\n",
       " 'test/15326',\n",
       " 'test/15327',\n",
       " 'test/15329',\n",
       " 'test/15335',\n",
       " 'test/15336',\n",
       " 'test/15337',\n",
       " 'test/15339',\n",
       " 'test/15341',\n",
       " 'test/15344',\n",
       " 'test/15345',\n",
       " 'test/15348',\n",
       " 'test/15349',\n",
       " 'test/15351',\n",
       " 'test/15352',\n",
       " 'test/15354',\n",
       " 'test/15356',\n",
       " 'test/15357',\n",
       " 'test/15359',\n",
       " 'test/15363',\n",
       " 'test/15364',\n",
       " 'test/15365',\n",
       " 'test/15366',\n",
       " 'test/15367',\n",
       " 'test/15368',\n",
       " 'test/15372',\n",
       " 'test/15375',\n",
       " 'test/15378',\n",
       " 'test/15379',\n",
       " 'test/15380',\n",
       " 'test/15383',\n",
       " 'test/15384',\n",
       " 'test/15386',\n",
       " 'test/15387',\n",
       " 'test/15388',\n",
       " 'test/15389',\n",
       " 'test/15391',\n",
       " 'test/15394',\n",
       " 'test/15396',\n",
       " 'test/15397',\n",
       " 'test/15400',\n",
       " 'test/15404',\n",
       " 'test/15406',\n",
       " 'test/15409',\n",
       " 'test/15410',\n",
       " 'test/15411',\n",
       " 'test/15413',\n",
       " 'test/15415',\n",
       " 'test/15416',\n",
       " 'test/15417',\n",
       " 'test/15420',\n",
       " 'test/15421',\n",
       " 'test/15424',\n",
       " 'test/15425',\n",
       " 'test/15427',\n",
       " 'test/15428',\n",
       " 'test/15429',\n",
       " 'test/15430',\n",
       " 'test/15431',\n",
       " 'test/15432',\n",
       " 'test/15436',\n",
       " 'test/15438',\n",
       " 'test/15441',\n",
       " 'test/15442',\n",
       " 'test/15444',\n",
       " 'test/15446',\n",
       " 'test/15447',\n",
       " 'test/15448',\n",
       " 'test/15449',\n",
       " 'test/15450',\n",
       " 'test/15451',\n",
       " 'test/15452',\n",
       " 'test/15453',\n",
       " 'test/15454',\n",
       " 'test/15455',\n",
       " 'test/15457',\n",
       " 'test/15459',\n",
       " 'test/15460',\n",
       " 'test/15462',\n",
       " 'test/15464',\n",
       " 'test/15467',\n",
       " 'test/15468',\n",
       " 'test/15471',\n",
       " 'test/15472',\n",
       " 'test/15476',\n",
       " 'test/15477',\n",
       " 'test/15478',\n",
       " 'test/15479',\n",
       " 'test/15481',\n",
       " 'test/15482',\n",
       " 'test/15483',\n",
       " 'test/15484',\n",
       " 'test/15485',\n",
       " 'test/15487',\n",
       " 'test/15489',\n",
       " 'test/15494',\n",
       " 'test/15495',\n",
       " 'test/15496',\n",
       " 'test/15500',\n",
       " 'test/15501',\n",
       " 'test/15503',\n",
       " 'test/15504',\n",
       " 'test/15510',\n",
       " 'test/15511',\n",
       " 'test/15515',\n",
       " 'test/15520',\n",
       " 'test/15521',\n",
       " 'test/15522',\n",
       " 'test/15523',\n",
       " 'test/15527',\n",
       " 'test/15528',\n",
       " 'test/15531',\n",
       " 'test/15532',\n",
       " 'test/15535',\n",
       " 'test/15536',\n",
       " 'test/15539',\n",
       " 'test/15540',\n",
       " 'test/15542',\n",
       " 'test/15543',\n",
       " 'test/15544',\n",
       " 'test/15545',\n",
       " 'test/15547',\n",
       " 'test/15548',\n",
       " 'test/15549',\n",
       " 'test/15550',\n",
       " 'test/15551',\n",
       " 'test/15552',\n",
       " 'test/15553',\n",
       " 'test/15556',\n",
       " 'test/15558',\n",
       " 'test/15559',\n",
       " 'test/15560',\n",
       " 'test/15561',\n",
       " 'test/15562',\n",
       " 'test/15563',\n",
       " 'test/15565',\n",
       " 'test/15566',\n",
       " 'test/15567',\n",
       " 'test/15568',\n",
       " 'test/15569',\n",
       " 'test/15570',\n",
       " 'test/15571',\n",
       " 'test/15572',\n",
       " 'test/15573',\n",
       " 'test/15574',\n",
       " 'test/15575',\n",
       " 'test/15578',\n",
       " 'test/15579',\n",
       " 'test/15580',\n",
       " 'test/15581',\n",
       " 'test/15582',\n",
       " 'test/15583',\n",
       " 'test/15584',\n",
       " 'test/15585',\n",
       " 'test/15590',\n",
       " 'test/15591',\n",
       " 'test/15593',\n",
       " 'test/15594',\n",
       " 'test/15595',\n",
       " 'test/15596',\n",
       " 'test/15597',\n",
       " 'test/15598',\n",
       " 'test/15600',\n",
       " 'test/15601',\n",
       " 'test/15602',\n",
       " 'test/15603',\n",
       " 'test/15605',\n",
       " 'test/15607',\n",
       " 'test/15610',\n",
       " 'test/15613',\n",
       " 'test/15615',\n",
       " 'test/15616',\n",
       " 'test/15617',\n",
       " 'test/15618',\n",
       " 'test/15620',\n",
       " 'test/15621',\n",
       " 'test/15623',\n",
       " 'test/15624',\n",
       " 'test/15625',\n",
       " 'test/15626',\n",
       " 'test/15629',\n",
       " 'test/15632',\n",
       " 'test/15634',\n",
       " 'test/15636',\n",
       " 'test/15637',\n",
       " 'test/15639',\n",
       " 'test/15640',\n",
       " 'test/15641',\n",
       " 'test/15642',\n",
       " 'test/15643',\n",
       " 'test/15646',\n",
       " 'test/15648',\n",
       " 'test/15649',\n",
       " 'test/15651',\n",
       " 'test/15653',\n",
       " 'test/15655',\n",
       " 'test/15656',\n",
       " 'test/15664',\n",
       " 'test/15666',\n",
       " 'test/15667',\n",
       " 'test/15668',\n",
       " 'test/15669',\n",
       " 'test/15672',\n",
       " 'test/15674',\n",
       " 'test/15675',\n",
       " 'test/15676',\n",
       " 'test/15677',\n",
       " 'test/15679',\n",
       " 'test/15680',\n",
       " 'test/15682',\n",
       " 'test/15686',\n",
       " 'test/15688',\n",
       " 'test/15689',\n",
       " 'test/15691',\n",
       " 'test/15692',\n",
       " 'test/15694',\n",
       " 'test/15695',\n",
       " 'test/15696',\n",
       " 'test/15698',\n",
       " 'test/15702',\n",
       " 'test/15703',\n",
       " 'test/15704',\n",
       " 'test/15707',\n",
       " 'test/15708',\n",
       " 'test/15709',\n",
       " 'test/15710',\n",
       " 'test/15713',\n",
       " 'test/15715',\n",
       " 'test/15717',\n",
       " 'test/15719',\n",
       " 'test/15720',\n",
       " 'test/15721',\n",
       " 'test/15723',\n",
       " 'test/15725',\n",
       " 'test/15726',\n",
       " 'test/15727',\n",
       " 'test/15728',\n",
       " 'test/15729',\n",
       " 'test/15732',\n",
       " 'test/15733',\n",
       " 'test/15736',\n",
       " 'test/15737',\n",
       " 'test/15739',\n",
       " 'test/15742',\n",
       " 'test/15749',\n",
       " 'test/15751',\n",
       " 'test/15753',\n",
       " 'test/15757',\n",
       " 'test/15759',\n",
       " 'test/15762',\n",
       " 'test/15767',\n",
       " 'test/15768',\n",
       " 'test/15769',\n",
       " 'test/15772',\n",
       " 'test/15777',\n",
       " 'test/15778',\n",
       " 'test/15780',\n",
       " 'test/15782',\n",
       " 'test/15785',\n",
       " 'test/15790',\n",
       " 'test/15793',\n",
       " 'test/15797',\n",
       " 'test/15798',\n",
       " 'test/15800',\n",
       " 'test/15801',\n",
       " 'test/15803',\n",
       " 'test/15804',\n",
       " 'test/15805',\n",
       " 'test/15807',\n",
       " 'test/15808',\n",
       " 'test/15810',\n",
       " 'test/15811',\n",
       " 'test/15816',\n",
       " 'test/15817',\n",
       " 'test/15819',\n",
       " 'test/15821',\n",
       " 'test/15822',\n",
       " 'test/15823',\n",
       " 'test/15829',\n",
       " 'test/15831',\n",
       " 'test/15832',\n",
       " 'test/15833',\n",
       " 'test/15834',\n",
       " 'test/15836',\n",
       " 'test/15838',\n",
       " 'test/15840',\n",
       " 'test/15841',\n",
       " 'test/15842',\n",
       " 'test/15844',\n",
       " 'test/15845',\n",
       " 'test/15846',\n",
       " 'test/15847',\n",
       " 'test/15851',\n",
       " 'test/15852',\n",
       " 'test/15853',\n",
       " 'test/15854',\n",
       " 'test/15855',\n",
       " 'test/15856',\n",
       " 'test/15858',\n",
       " 'test/15859',\n",
       " 'test/15860',\n",
       " 'test/15861',\n",
       " 'test/15863',\n",
       " 'test/15864',\n",
       " 'test/15865',\n",
       " 'test/15866',\n",
       " 'test/15867',\n",
       " 'test/15868',\n",
       " 'test/15869',\n",
       " 'test/15870',\n",
       " 'test/15871',\n",
       " 'test/15872',\n",
       " 'test/15874',\n",
       " 'test/15875',\n",
       " 'test/15876',\n",
       " 'test/15877',\n",
       " 'test/15878',\n",
       " 'test/15879',\n",
       " 'test/15881',\n",
       " 'test/15885',\n",
       " 'test/15886',\n",
       " 'test/15888',\n",
       " 'test/15889',\n",
       " 'test/15890',\n",
       " 'test/15892',\n",
       " 'test/15893',\n",
       " 'test/15894',\n",
       " 'test/15895',\n",
       " 'test/15896',\n",
       " 'test/15897',\n",
       " 'test/15898',\n",
       " 'test/15899',\n",
       " 'test/15900',\n",
       " 'test/15901',\n",
       " 'test/15902',\n",
       " 'test/15903',\n",
       " 'test/15904',\n",
       " 'test/15906',\n",
       " 'test/15908',\n",
       " 'test/15909',\n",
       " 'test/15910',\n",
       " 'test/15911',\n",
       " 'test/15912',\n",
       " 'test/15913',\n",
       " 'test/15914',\n",
       " 'test/15916',\n",
       " 'test/15917',\n",
       " 'test/15918',\n",
       " 'test/15920',\n",
       " 'test/15921',\n",
       " 'test/15922',\n",
       " 'test/15923',\n",
       " 'test/15924',\n",
       " 'test/15925',\n",
       " 'test/15927',\n",
       " 'test/15928',\n",
       " 'test/15929',\n",
       " 'test/15930',\n",
       " 'test/15932',\n",
       " 'test/15933',\n",
       " 'test/15934',\n",
       " 'test/15937',\n",
       " 'test/15939',\n",
       " 'test/15942',\n",
       " 'test/15944',\n",
       " 'test/15949',\n",
       " 'test/15950',\n",
       " 'test/15951',\n",
       " 'test/15952',\n",
       " 'test/15953',\n",
       " 'test/15956',\n",
       " 'test/15959',\n",
       " 'test/15960',\n",
       " 'test/15961',\n",
       " 'test/15963',\n",
       " 'test/15964',\n",
       " 'test/15967',\n",
       " 'test/15968',\n",
       " 'test/15969',\n",
       " 'test/15970',\n",
       " 'test/15973',\n",
       " 'test/15975',\n",
       " 'test/15976',\n",
       " 'test/15977',\n",
       " 'test/15978',\n",
       " 'test/15979',\n",
       " 'test/15980',\n",
       " 'test/15981',\n",
       " 'test/15984',\n",
       " 'test/15985',\n",
       " 'test/15987',\n",
       " 'test/15988',\n",
       " 'test/15989',\n",
       " 'test/15993',\n",
       " 'test/15995',\n",
       " 'test/15996',\n",
       " 'test/15997',\n",
       " 'test/15999',\n",
       " 'test/16002',\n",
       " 'test/16003',\n",
       " 'test/16004',\n",
       " 'test/16005',\n",
       " 'test/16006',\n",
       " 'test/16007',\n",
       " 'test/16009',\n",
       " 'test/16012',\n",
       " 'test/16013',\n",
       " 'test/16014',\n",
       " 'test/16015',\n",
       " 'test/16016',\n",
       " 'test/16021',\n",
       " 'test/16022',\n",
       " 'test/16023',\n",
       " 'test/16026',\n",
       " 'test/16029',\n",
       " 'test/16030',\n",
       " 'test/16033',\n",
       " 'test/16037',\n",
       " 'test/16040',\n",
       " 'test/16041',\n",
       " 'test/16045',\n",
       " 'test/16052',\n",
       " 'test/16053',\n",
       " 'test/16055',\n",
       " 'test/16063',\n",
       " 'test/16066',\n",
       " 'test/16067',\n",
       " 'test/16068',\n",
       " 'test/16069',\n",
       " 'test/16071',\n",
       " 'test/16072',\n",
       " 'test/16074',\n",
       " 'test/16075',\n",
       " 'test/16076',\n",
       " 'test/16077',\n",
       " 'test/16079',\n",
       " 'test/16080',\n",
       " 'test/16083',\n",
       " 'test/16086',\n",
       " 'test/16088',\n",
       " 'test/16091',\n",
       " 'test/16093',\n",
       " 'test/16094',\n",
       " 'test/16095',\n",
       " 'test/16096',\n",
       " 'test/16097',\n",
       " 'test/16098',\n",
       " 'test/16099',\n",
       " 'test/16100',\n",
       " 'test/16103',\n",
       " 'test/16106',\n",
       " 'test/16107',\n",
       " 'test/16108',\n",
       " 'test/16110',\n",
       " 'test/16111',\n",
       " 'test/16112',\n",
       " 'test/16115',\n",
       " 'test/16117',\n",
       " 'test/16118',\n",
       " 'test/16119',\n",
       " 'test/16120',\n",
       " 'test/16122',\n",
       " 'test/16123',\n",
       " 'test/16125',\n",
       " 'test/16126',\n",
       " 'test/16130',\n",
       " 'test/16133',\n",
       " 'test/16134',\n",
       " 'test/16136',\n",
       " 'test/16139',\n",
       " 'test/16140',\n",
       " 'test/16141',\n",
       " 'test/16142',\n",
       " 'test/16143',\n",
       " 'test/16144',\n",
       " 'test/16145',\n",
       " 'test/16146',\n",
       " 'test/16147',\n",
       " 'test/16148',\n",
       " 'test/16149',\n",
       " 'test/16150',\n",
       " 'test/16152',\n",
       " 'test/16155',\n",
       " 'test/16158',\n",
       " 'test/16159',\n",
       " 'test/16161',\n",
       " 'test/16162',\n",
       " 'test/16163',\n",
       " 'test/16164',\n",
       " 'test/16166',\n",
       " 'test/16170',\n",
       " 'test/16171',\n",
       " 'test/16172',\n",
       " 'test/16173',\n",
       " 'test/16175',\n",
       " 'test/16176',\n",
       " 'test/16177',\n",
       " 'test/16179',\n",
       " 'test/16180',\n",
       " 'test/16185',\n",
       " 'test/16188',\n",
       " 'test/16189',\n",
       " 'test/16190',\n",
       " 'test/16193',\n",
       " 'test/16194',\n",
       " 'test/16195',\n",
       " 'test/16196',\n",
       " 'test/16197',\n",
       " 'test/16200',\n",
       " 'test/16201',\n",
       " 'test/16202',\n",
       " 'test/16203',\n",
       " 'test/16206',\n",
       " 'test/16207',\n",
       " 'test/16210',\n",
       " 'test/16211',\n",
       " 'test/16212',\n",
       " 'test/16213',\n",
       " 'test/16214',\n",
       " 'test/16215',\n",
       " 'test/16216',\n",
       " 'test/16219',\n",
       " 'test/16221',\n",
       " 'test/16223',\n",
       " 'test/16225',\n",
       " 'test/16226',\n",
       " 'test/16228',\n",
       " 'test/16230',\n",
       " 'test/16232',\n",
       " 'test/16233',\n",
       " 'test/16234',\n",
       " 'test/16236',\n",
       " 'test/16238',\n",
       " 'test/16241',\n",
       " 'test/16243',\n",
       " 'test/16244',\n",
       " 'test/16246',\n",
       " 'test/16247',\n",
       " 'test/16248',\n",
       " 'test/16250',\n",
       " 'test/16251',\n",
       " 'test/16252',\n",
       " 'test/16255',\n",
       " 'test/16256',\n",
       " 'test/16257',\n",
       " 'test/16258',\n",
       " 'test/16260',\n",
       " 'test/16262',\n",
       " 'test/16263',\n",
       " 'test/16264',\n",
       " 'test/16265',\n",
       " 'test/16266',\n",
       " 'test/16268',\n",
       " 'test/16269',\n",
       " 'test/16270',\n",
       " 'test/16271',\n",
       " 'test/16274',\n",
       " 'test/16275',\n",
       " 'test/16277',\n",
       " 'test/16278',\n",
       " 'test/16279',\n",
       " 'test/16281',\n",
       " 'test/16282',\n",
       " 'test/16283',\n",
       " 'test/16284',\n",
       " 'test/16285',\n",
       " 'test/16286',\n",
       " 'test/16287',\n",
       " 'test/16288',\n",
       " 'test/16289',\n",
       " 'test/16291',\n",
       " 'test/16294',\n",
       " 'test/16297',\n",
       " 'test/16298',\n",
       " 'test/16299',\n",
       " 'test/16300',\n",
       " 'test/16301',\n",
       " 'test/16302',\n",
       " 'test/16303',\n",
       " 'test/16304',\n",
       " 'test/16307',\n",
       " 'test/16310',\n",
       " 'test/16311',\n",
       " 'test/16312',\n",
       " 'test/16314',\n",
       " 'test/16315',\n",
       " 'test/16316',\n",
       " 'test/16317',\n",
       " 'test/16318',\n",
       " 'test/16319',\n",
       " 'test/16320',\n",
       " 'test/16324',\n",
       " 'test/16327',\n",
       " 'test/16331',\n",
       " 'test/16332',\n",
       " 'test/16336',\n",
       " 'test/16337',\n",
       " 'test/16339',\n",
       " 'test/16342',\n",
       " 'test/16343',\n",
       " 'test/16346',\n",
       " 'test/16347',\n",
       " 'test/16348',\n",
       " 'test/16350',\n",
       " 'test/16354',\n",
       " 'test/16357',\n",
       " 'test/16359',\n",
       " 'test/16360',\n",
       " 'test/16362',\n",
       " 'test/16363',\n",
       " 'test/16365',\n",
       " 'test/16366',\n",
       " 'test/16367',\n",
       " 'test/16369',\n",
       " 'test/16370',\n",
       " 'test/16371',\n",
       " 'test/16372',\n",
       " 'test/16374',\n",
       " 'test/16376',\n",
       " 'test/16377',\n",
       " 'test/16379',\n",
       " 'test/16380',\n",
       " 'test/16383',\n",
       " 'test/16385',\n",
       " 'test/16386',\n",
       " 'test/16388',\n",
       " 'test/16390',\n",
       " 'test/16392',\n",
       " 'test/16393',\n",
       " 'test/16394',\n",
       " 'test/16395',\n",
       " 'test/16396',\n",
       " 'test/16398',\n",
       " 'test/16399',\n",
       " 'test/16400',\n",
       " 'test/16401',\n",
       " 'test/16402',\n",
       " 'test/16403',\n",
       " 'test/16404',\n",
       " 'test/16405',\n",
       " 'test/16406',\n",
       " 'test/16407',\n",
       " 'test/16409',\n",
       " 'test/16410',\n",
       " 'test/16415',\n",
       " 'test/16417',\n",
       " 'test/16418',\n",
       " 'test/16419',\n",
       " 'test/16420',\n",
       " 'test/16421',\n",
       " 'test/16422',\n",
       " 'test/16424',\n",
       " 'test/16426',\n",
       " 'test/16427',\n",
       " 'test/16428',\n",
       " 'test/16429',\n",
       " 'test/16430',\n",
       " 'test/16432',\n",
       " 'test/16433',\n",
       " 'test/16434',\n",
       " 'test/16437',\n",
       " 'test/16438',\n",
       " 'test/16440',\n",
       " 'test/16441',\n",
       " 'test/16442',\n",
       " 'test/16443',\n",
       " 'test/16444',\n",
       " 'test/16448',\n",
       " 'test/16449',\n",
       " 'test/16450',\n",
       " 'test/16454',\n",
       " 'test/16457',\n",
       " 'test/16458',\n",
       " 'test/16459',\n",
       " 'test/16460',\n",
       " 'test/16461',\n",
       " 'test/16463',\n",
       " 'test/16465',\n",
       " 'test/16468',\n",
       " 'test/16469',\n",
       " 'test/16470',\n",
       " 'test/16471',\n",
       " 'test/16472',\n",
       " 'test/16473',\n",
       " 'test/16475',\n",
       " 'test/16476',\n",
       " 'test/16478',\n",
       " 'test/16479',\n",
       " 'test/16480',\n",
       " 'test/16481',\n",
       " 'test/16483',\n",
       " 'test/16486',\n",
       " 'test/16487',\n",
       " 'test/16488',\n",
       " 'test/16490',\n",
       " 'test/16492',\n",
       " 'test/16493',\n",
       " 'test/16495',\n",
       " 'test/16496',\n",
       " 'test/16499',\n",
       " 'test/16502',\n",
       " 'test/16505',\n",
       " 'test/16510',\n",
       " 'test/16512',\n",
       " 'test/16513',\n",
       " 'test/16518',\n",
       " 'test/16519',\n",
       " 'test/16521',\n",
       " 'test/16522',\n",
       " 'test/16523',\n",
       " 'test/16525',\n",
       " 'test/16527',\n",
       " 'test/16530',\n",
       " 'test/16531',\n",
       " 'test/16533',\n",
       " 'test/16538',\n",
       " 'test/16539',\n",
       " 'test/16545',\n",
       " 'test/16546',\n",
       " 'test/16549',\n",
       " 'test/16551',\n",
       " 'test/16554',\n",
       " 'test/16555',\n",
       " 'test/16561',\n",
       " 'test/16563',\n",
       " 'test/16564',\n",
       " 'test/16565',\n",
       " 'test/16568',\n",
       " 'test/16569',\n",
       " 'test/16570',\n",
       " 'test/16574',\n",
       " 'test/16577',\n",
       " 'test/16581',\n",
       " 'test/16583',\n",
       " 'test/16584',\n",
       " 'test/16585',\n",
       " 'test/16587',\n",
       " 'test/16588',\n",
       " 'test/16589',\n",
       " 'test/16590',\n",
       " 'test/16591',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "006be6d61153b59e5f9725e67d268e9f62c78339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load 10k reuters news documents \n",
    "len(reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "ca9bda17034229327afd7d6cc769425d81bf76e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reaching\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view text from one document \n",
    "reuters.raw(fileids=['test/14826'])[0:201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "adf15a9fb24d881a1a2c4f11a955cd1d75a59106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINA DAILY SAYS VERMIN EAT 712 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showed vermin consume between seven and 12 pct of Chinas grain\n",
      "  stocks the China Daily said\n",
      "      It also said that each year 1575 mln tonnes or 25 pct of\n",
      "  Chinas fruit output are left to rot and 21 mln tonnes or up\n",
      "  to 30 pct of its vegetables The paper blamed the waste on\n",
      "  inadequate storage and bad preservation methods\n",
      "      It said the government had launched a national programme to\n",
      "  reduce waste calling for improved technology in storage and\n",
      "  preservation and greater production of additives The paper\n",
      "  gave no further details\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation from all DOCs \n",
    "exclude = set(string.punctuation)\n",
    "alldocslist = []\n",
    "\n",
    "for index, i in  enumerate(reuters.fileids()):\n",
    "    text = reuters.raw(fileids=[i])\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    alldocslist.append(text)\n",
    "    \n",
    "print(alldocslist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHINA', 'DAILY', 'SAYS', 'VERMIN', 'EAT', '712', 'PCT', 'GRAIN', 'STOCKS', 'A', 'survey', 'of', '19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'Chinas', 'grain', 'stocks', 'the', 'China', 'Daily', 'said', 'It', 'also', 'said', 'that', 'each', 'year', '1575', 'mln', 'tonnes', 'or', '25', 'pct', 'of', 'Chinas', 'fruit', 'output', 'are', 'left', 'to', 'rot', 'and', '21', 'mln', 'tonnes', 'or', 'up', 'to', '30', 'pct', 'of', 'its', 'vegetables', 'The', 'paper', 'blamed', 'the', 'waste', 'on', 'inadequate', 'storage', 'and', 'bad', 'preservation', 'methods', 'It', 'said', 'the', 'government', 'had', 'launched', 'a', 'national', 'programme', 'to', 'reduce', 'waste', 'calling', 'for', 'improved', 'technology', 'in', 'storage', 'and', 'preservation', 'and', 'greater', 'production', 'of', 'additives', 'The', 'paper', 'gave', 'no', 'further', 'details']\n"
     ]
    }
   ],
   "source": [
    "#tokenize words in all DOCS \n",
    "plot_data = [[]] * len(alldocslist)\n",
    "\n",
    "for doc in alldocslist:\n",
    "    text = doc\n",
    "    tokentext = word_tokenize(text)\n",
    "    plot_data[index].append(tokentext)\n",
    "    \n",
    "print(plot_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "fd5c50507cec89313952d57aad07e9d0842ff9e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JAPAN',\n",
       " 'TO',\n",
       " 'REVISE',\n",
       " 'LONGTERM',\n",
       " 'ENERGY',\n",
       " 'DEMAND',\n",
       " 'DOWNWARDS',\n",
       " 'The',\n",
       " 'Ministry',\n",
       " 'of']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Navigation: first index gives all documents, second index gives specific document, third index gives words of that doc\n",
    "plot_data[0][2][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "7f2c197cf782bce0102488515a65ba735f656587"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['japan',\n",
       " 'to',\n",
       " 'revise',\n",
       " 'longterm',\n",
       " 'energy',\n",
       " 'demand',\n",
       " 'downwards',\n",
       " 'the',\n",
       " 'ministry',\n",
       " 'of']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#make all words lower case for all docs \n",
    "for x in range(len(reuters.fileids())):\n",
    "    lowers = [word.lower() for word in plot_data[0][x]]\n",
    "    plot_data[0][x] = lowers\n",
    "\n",
    "plot_data[0][2][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dhanendra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('stopwords')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "6e74f62ed696191d32d14fb815e0e1ab6f8b393b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['china',\n",
       " 'daily',\n",
       " 'says',\n",
       " 'vermin',\n",
       " 'eat',\n",
       " '712',\n",
       " 'pct',\n",
       " 'grain',\n",
       " 'stocks',\n",
       " 'survey']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words from all docs \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for x in range(len(reuters.fileids())):\n",
    "    filtered_sentence = [w for w in plot_data[0][x] if not w in stop_words]\n",
    "    plot_data[0][x] = filtered_sentence\n",
    "\n",
    "plot_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "d47eebfd211946959f31d9606f7f8c20f70f3303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ltaha',\n",
       " 'automot',\n",
       " 'technolog',\n",
       " 'corp',\n",
       " 'year',\n",
       " 'net',\n",
       " 'shr',\n",
       " '43',\n",
       " 'ct',\n",
       " 'vs']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stem words EXAMPLE (could try others/lemmers )\n",
    "\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = [snowball_stemmer.stem(w) for w in filtered_sentence]\n",
    "stemmed_sentence[0:10]\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = [ porter_stemmer.stem(w) for w in filtered_sentence]\n",
    "stemmed_sentence[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db78d4e0b4f69fef8560ad8575d3ea9e17980d7f"
   },
   "source": [
    "# PART II: CREATING THE INVERSE-INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "2a0605e9d7a1dbd52ebfea44ba7ded39fc53767c"
   },
   "outputs": [],
   "source": [
    "# Create inverse index which gives document number for each document and where word appears\n",
    "\n",
    "#first we need to create a list of all words \n",
    "l = plot_data[0]\n",
    "flatten = [item for sublist in l for item in sublist]\n",
    "words = flatten\n",
    "wordsunique = set(words)\n",
    "wordsunique = list(wordsunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "49efc97ad7151065564bb516cd3a53ff9297c1f1"
   },
   "outputs": [],
   "source": [
    "# create functions for TD-IDF / BM25\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.count(word) / len(doc)\n",
    "\n",
    "def n_containing(word, doclist):\n",
    "    return sum(1 for doc in doclist if word in doc)\n",
    "\n",
    "def idf(word, doclist):\n",
    "    return math.log(len(doclist) / (0.01 + n_containing(word, doclist)))\n",
    "\n",
    "def tfidf(word, doc, doclist):\n",
    "    return (tf(word, doc) * idf(word, doclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "52c04e635a0c2519139a48e32f6f9f388bbe99d6"
   },
   "outputs": [],
   "source": [
    "# Create dictonary of words\n",
    "# THIS ONE-TIME INDEXING IS THE MOST PROCESSOR-INTENSIVE STEP AND WILL TAKE TIME TO RUN (BUT ONLY NEEDS TO BE RUN ONCE)\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "plottest = plot_data[0][0:1000]\n",
    "\n",
    "worddic = {}\n",
    "\n",
    "for doc in plottest:\n",
    "    for word in wordsunique:\n",
    "        if word in doc:\n",
    "            word = str(word)\n",
    "            index = plottest.index(doc)\n",
    "            positions = list(np.where(np.array(plottest[index]) == word)[0])\n",
    "            idfs = tfidf(word,doc,plottest)\n",
    "            try:\n",
    "                worddic[word].append([index,positions,idfs])\n",
    "            except:\n",
    "                worddic[word] = []\n",
    "                worddic[word].append([index,positions,idfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "bb59b82d32b95214e512ff468113c12484588498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, [0, 23], 0.1131500878815288],\n",
       " [13, [0], 0.06694713532990454],\n",
       " [14, [160], 0.013213250394060107],\n",
       " [28, [51], 0.05821490028687352],\n",
       " [40, [3, 15, 59, 79], 0.14740653650621185],\n",
       " [236, [86], 0.04414096834938761],\n",
       " [281, [70], 0.0565750439407644],\n",
       " [293, [13, 21], 0.11642980057374704],\n",
       " [302, [33], 0.059952658504392124],\n",
       " [342, [55, 146], 0.05391715597039292],\n",
       " [567, [2], 0.06925565723783228],\n",
       " [569, [1014, 1072, 1221], 0.009248261212112677],\n",
       " [612, [20], 0.01998421950146404],\n",
       " [710, [0, 7, 34], 0.17464470086062053],\n",
       " [720, [0, 16], 0.23628400704672192],\n",
       " [721, [0, 6, 27, 78, 82], 0.2028701070603168],\n",
       " [733, [179], 0.021595850106420823],\n",
       " [736, [0, 5, 21, 83], 0.13732745708698368]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the index creates a dic with each word as a KEY and a list of doc indexs, word positions, and td-idf score as VALUES\n",
    "worddic['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "812e14b05aaca705a2f5b5b4228cdfb14a71b198"
   },
   "outputs": [],
   "source": [
    "# pickel (save) the dictonary to avoid re-calculating\n",
    "np.save('worddic_1000.npy', worddic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2bba44192406f7e4118fedc0710d2ac46154a568"
   },
   "source": [
    "# PART III: The Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "2147470ee4e617ed9bd86dd4cc8ac28a8624cd8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indonesia', 'crude', 'palm', 'oil']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create word search which takes multiple words and finds documents that contain both along with metrics for ranking:\n",
    "\n",
    "    ## (1) Number of occruances of search words \n",
    "    ## (2) TD-IDF score for search words \n",
    "    ## (3) Percentage of search terms\n",
    "    ## (4) Word ordering score \n",
    "    ## (5) Exact match bonus \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def search(searchsentence):\n",
    "    try:\n",
    "        # split sentence into individual words \n",
    "        searchsentence = searchsentence.lower()\n",
    "        try:\n",
    "            words = searchsentence.split(' ')\n",
    "        except:\n",
    "            words = list(words)\n",
    "        enddic = {}\n",
    "        idfdic = {}\n",
    "        closedic = {}\n",
    "        \n",
    "        # remove words if not in worddic \n",
    "        realwords = []\n",
    "        for word in words:\n",
    "            if word in list(worddic.keys()):\n",
    "                realwords.append(word)  \n",
    "        words = realwords\n",
    "        numwords = len(words)\n",
    "        \n",
    "        # make metric of number of occurances of all words in each doc & largest total IDF \n",
    "        for word in words:\n",
    "            for indpos in worddic[word]:\n",
    "                index = indpos[0]\n",
    "                amount = len(indpos[1])\n",
    "                idfscore = indpos[2]\n",
    "                enddic[index] = amount\n",
    "                idfdic[index] = idfscore\n",
    "                fullcount_order = sorted(enddic.items(), key=lambda x:x[1], reverse=True)\n",
    "                fullidf_order = sorted(idfdic.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "                \n",
    "        # make metric of what percentage of words appear in each doc\n",
    "        combo = []\n",
    "        alloptions = {k: worddic.get(k, None) for k in (words)}\n",
    "        for worddex in list(alloptions.values()):\n",
    "            for indexpos in worddex:\n",
    "                for indexz in indexpos:\n",
    "                    combo.append(indexz)\n",
    "        comboindex = combo[::3]\n",
    "        combocount = Counter(comboindex)\n",
    "        for key in combocount:\n",
    "            combocount[key] = combocount[key] / numwords\n",
    "        combocount_order = sorted(combocount.items(), key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "        # make metric for if words appear in same order as in search\n",
    "        if len(words) > 1:\n",
    "            x = []\n",
    "            y = []\n",
    "            for record in [worddic[z] for z in words]:\n",
    "                for index in record:\n",
    "                     x.append(index[0])\n",
    "            for i in x:\n",
    "                if x.count(i) > 1:\n",
    "                    y.append(i)\n",
    "            y = list(set(y))\n",
    "\n",
    "            closedic = {}\n",
    "            for wordbig in [worddic[x] for x in words]:\n",
    "                for record in wordbig:\n",
    "                    if record[0] in y:\n",
    "                        index = record[0]\n",
    "                        positions = record[1]\n",
    "                        try:\n",
    "                            closedic[index].append(positions)\n",
    "                        except:\n",
    "                            closedic[index] = []\n",
    "                            closedic[index].append(positions)\n",
    "\n",
    "            x = 0\n",
    "            fdic = {}\n",
    "            for index in y:\n",
    "                csum = []\n",
    "                for seqlist in closedic[index]:\n",
    "                    while x > 0:\n",
    "                        secondlist = seqlist\n",
    "                        x = 0\n",
    "                        sol = [1 for i in firstlist if i + 1 in secondlist]\n",
    "                        csum.append(sol)\n",
    "                        fsum = [item for sublist in csum for item in sublist]\n",
    "                        fsum = sum(fsum)\n",
    "                        fdic[index] = fsum\n",
    "                        fdic_order = sorted(fdic.items(), key=lambda x:x[1], reverse=True)\n",
    "                    while x == 0:\n",
    "                        firstlist = seqlist\n",
    "                        x = x + 1\n",
    "        else:\n",
    "            fdic_order = 0\n",
    "                    \n",
    "        # also the one above should be given a big boost if ALL found together \n",
    "           \n",
    "        \n",
    "        #could make another metric for if they are not next to each other but still close \n",
    "        \n",
    "        \n",
    "        return(searchsentence,words,fullcount_order,combocount_order,fullidf_order,fdic_order)\n",
    "    \n",
    "    except:\n",
    "        return(\"\")\n",
    "\n",
    "\n",
    "search('indonesia crude palm oil')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "019fcbd773b5c848507464b3d20c8ce80b5be31f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crude', 'palm', 'oil']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 return will give back the search term, the rest will give back metrics (see above)\n",
    "\n",
    "search('indonesia crude palm oil')[1][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "1969ddc3a6ccd58339a4cb22241a92334609d0d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search term</th>\n",
       "      <th>actual_words_searched</th>\n",
       "      <th>num_occur</th>\n",
       "      <th>percentage_of_terms</th>\n",
       "      <th>td-idf</th>\n",
       "      <th>word_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>china daily says what</td>\n",
       "      <td>[china, daily, says]</td>\n",
       "      <td>[(183, 5), (40, 4), (569, 3), (710, 3), (342, ...</td>\n",
       "      <td>[(1, 1.0), (13, 0.6666666666666666), (14, 0.66...</td>\n",
       "      <td>[(675, 0.5095658223243495), (135, 0.4367707048...</td>\n",
       "      <td>[(1, 3), (293, 1), (720, 1), (721, 1), (736, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indonesia crude palm oil</td>\n",
       "      <td>[indonesia, crude, palm, oil]</td>\n",
       "      <td>[(33, 13), (621, 12), (34, 11), (209, 8), (123...</td>\n",
       "      <td>[(4, 1.0), (6, 1.0), (209, 0.5), (281, 0.5), (...</td>\n",
       "      <td>[(762, 0.48707909813666866), (266, 0.434203698...</td>\n",
       "      <td>[(34, 6), (4, 5), (660, 5), (6, 4), (268, 2), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price of nickel</td>\n",
       "      <td>[price, nickel]</td>\n",
       "      <td>[(572, 19), (639, 8), (108, 7), (148, 7), (736...</td>\n",
       "      <td>[(724, 1.0), (4, 0.5), (7, 0.5), (20, 0.5), (2...</td>\n",
       "      <td>[(50, 0.24460301234499893), (537, 0.2066299280...</td>\n",
       "      <td>[(724, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>north yemen sugar</td>\n",
       "      <td>[north, yemen, sugar]</td>\n",
       "      <td>[(700, 12), (96, 8), (494, 7), (296, 6), (525,...</td>\n",
       "      <td>[(30, 1.0), (758, 1.0), (47, 0.666666666666666...</td>\n",
       "      <td>[(494, 0.3808351739278394), (30, 0.35115970582...</td>\n",
       "      <td>[(758, 2), (30, 2), (851, 0), (47, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nippon steel</td>\n",
       "      <td>[nippon, steel]</td>\n",
       "      <td>[(40, 9), (253, 8), (444, 7), (223, 2), (435, ...</td>\n",
       "      <td>[(40, 1.0), (123, 0.5), (223, 0.5), (253, 0.5)...</td>\n",
       "      <td>[(223, 0.5682589478261134), (40, 0.42228417223...</td>\n",
       "      <td>[(40, 5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>china</td>\n",
       "      <td>[china]</td>\n",
       "      <td>[(721, 5), (40, 4), (736, 4), (569, 3), (710, ...</td>\n",
       "      <td>[(1, 1.0), (13, 1.0), (14, 1.0), (28, 1.0), (4...</td>\n",
       "      <td>[(720, 0.23628400704672192), (721, 0.202870107...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gold</td>\n",
       "      <td>[gold]</td>\n",
       "      <td>[(997, 6), (20, 5), (797, 5), (341, 4), (347, ...</td>\n",
       "      <td>[(8, 1.0), (12, 1.0), (20, 1.0), (32, 1.0), (2...</td>\n",
       "      <td>[(304, 0.30902054113001826), (20, 0.2575171176...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trade</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>[(0, 15), (169, 10), (544, 10), (761, 8), (273...</td>\n",
       "      <td>[(285, 2.0), (701, 2.0), (713, 2.0), (923, 2.0...</td>\n",
       "      <td>[(223, 0.24728127372797265), (449, 0.247281273...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                search term          actual_words_searched  \\\n",
       "0     china daily says what           [china, daily, says]   \n",
       "1  indonesia crude palm oil  [indonesia, crude, palm, oil]   \n",
       "2           price of nickel                [price, nickel]   \n",
       "3         north yemen sugar          [north, yemen, sugar]   \n",
       "4              nippon steel                [nippon, steel]   \n",
       "5                     china                        [china]   \n",
       "6                      gold                         [gold]   \n",
       "7                     trade                        [trade]   \n",
       "\n",
       "                                           num_occur  \\\n",
       "0  [(183, 5), (40, 4), (569, 3), (710, 3), (342, ...   \n",
       "1  [(33, 13), (621, 12), (34, 11), (209, 8), (123...   \n",
       "2  [(572, 19), (639, 8), (108, 7), (148, 7), (736...   \n",
       "3  [(700, 12), (96, 8), (494, 7), (296, 6), (525,...   \n",
       "4  [(40, 9), (253, 8), (444, 7), (223, 2), (435, ...   \n",
       "5  [(721, 5), (40, 4), (736, 4), (569, 3), (710, ...   \n",
       "6  [(997, 6), (20, 5), (797, 5), (341, 4), (347, ...   \n",
       "7  [(0, 15), (169, 10), (544, 10), (761, 8), (273...   \n",
       "\n",
       "                                 percentage_of_terms  \\\n",
       "0  [(1, 1.0), (13, 0.6666666666666666), (14, 0.66...   \n",
       "1  [(4, 1.0), (6, 1.0), (209, 0.5), (281, 0.5), (...   \n",
       "2  [(724, 1.0), (4, 0.5), (7, 0.5), (20, 0.5), (2...   \n",
       "3  [(30, 1.0), (758, 1.0), (47, 0.666666666666666...   \n",
       "4  [(40, 1.0), (123, 0.5), (223, 0.5), (253, 0.5)...   \n",
       "5  [(1, 1.0), (13, 1.0), (14, 1.0), (28, 1.0), (4...   \n",
       "6  [(8, 1.0), (12, 1.0), (20, 1.0), (32, 1.0), (2...   \n",
       "7  [(285, 2.0), (701, 2.0), (713, 2.0), (923, 2.0...   \n",
       "\n",
       "                                              td-idf  \\\n",
       "0  [(675, 0.5095658223243495), (135, 0.4367707048...   \n",
       "1  [(762, 0.48707909813666866), (266, 0.434203698...   \n",
       "2  [(50, 0.24460301234499893), (537, 0.2066299280...   \n",
       "3  [(494, 0.3808351739278394), (30, 0.35115970582...   \n",
       "4  [(223, 0.5682589478261134), (40, 0.42228417223...   \n",
       "5  [(720, 0.23628400704672192), (721, 0.202870107...   \n",
       "6  [(304, 0.30902054113001826), (20, 0.2575171176...   \n",
       "7  [(223, 0.24728127372797265), (449, 0.247281273...   \n",
       "\n",
       "                                          word_order  \n",
       "0  [(1, 3), (293, 1), (720, 1), (721, 1), (736, 0...  \n",
       "1  [(34, 6), (4, 5), (660, 5), (6, 4), (268, 2), ...  \n",
       "2                                         [(724, 0)]  \n",
       "3             [(758, 2), (30, 2), (851, 0), (47, 0)]  \n",
       "4                                          [(40, 5)]  \n",
       "5                                                  0  \n",
       "6                                                  0  \n",
       "7                                                  0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save metrics to dataframe for use in ranking and machine learning \n",
    "result1 = search('china daily says what')\n",
    "result2 = search('indonesia crude palm oil')\n",
    "result3 = search('price of nickel')\n",
    "result4 = search('north yemen sugar')\n",
    "result5 = search('nippon steel')\n",
    "result6 = search('China')\n",
    "result7 = search('Gold')\n",
    "result8 = search('trade')\n",
    "df = pd.DataFrame([result1,result2,result3,result4,result5,result6,result7,result8])\n",
    "df.columns = ['search term', 'actual_words_searched','num_occur','percentage_of_terms','td-idf','word_order']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "d73df8c6ffbff3d6d9f665f85f9b98912a4e8a70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHINA DAILY SAYS VERMIN EAT 712 PCT GRAIN STOCKS\\n  A survey of 19 provinces and seven cities\\n  showed vermin consume between seven and 12 pct of Chinas grain\\n  stocks the China Daily said\\n      It also said that each year 1575 mln tonnes or 25 pct of\\n  Chinas fruit output are left to rot and 21 mln tonnes or up\\n  to 30 pct of its vegetables The paper blamed the waste on\\n  inadequate storage and bad preservation methods\\n      It said the government had launched a national programme to\\n  reduce waste calling for improved technology in storage and\\n  preservation and greater production of additives The paper\\n  gave no further details\\n  \\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look to see if the top documents seem to make sense\n",
    "\n",
    "alldocslist[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d35e72474705e00e8ac750979812c98aec65e5b4"
   },
   "source": [
    "# PART IV: Rank and return (rules based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "ada71696d2166bee882afe85a2b8fb2b4c10ff2a"
   },
   "outputs": [],
   "source": [
    "# create a simple (non-machine learning) rank and return function\n",
    "\n",
    "def rank(term):\n",
    "    results = search(term)\n",
    "\n",
    "    # get metrics \n",
    "    num_score = results[2]\n",
    "    per_score = results[3]\n",
    "    tfscore = results[4]\n",
    "    order_score = results[5]\n",
    "    \n",
    "    final_candidates = []\n",
    "\n",
    "    # rule1: if high word order score & 100% percentage terms then put at top position\n",
    "    try:\n",
    "        first_candidates = []\n",
    "\n",
    "        for candidates in order_score:\n",
    "            if candidates[1] > 1:\n",
    "                first_candidates.append(candidates[0])\n",
    "\n",
    "        second_candidates = []\n",
    "\n",
    "        for match_candidates in per_score:\n",
    "            if match_candidates[1] == 1:\n",
    "                second_candidates.append(match_candidates[0])\n",
    "            if match_candidates[1] == 1 and match_candidates[0] in first_candidates:\n",
    "                final_candidates.append(match_candidates[0])\n",
    "\n",
    "    # rule2: next add other word order score which are greater than 1 \n",
    "\n",
    "        t3_order = first_candidates[0:3]\n",
    "        for each in t3_order:\n",
    "            if each not in final_candidates:\n",
    "                final_candidates.insert(len(final_candidates),each)\n",
    "\n",
    "    # rule3: next add top td-idf results\n",
    "        final_candidates.insert(len(final_candidates),tfscore[0][0])\n",
    "        final_candidates.insert(len(final_candidates),tfscore[1][0])\n",
    "\n",
    "    # rule4: next add other high percentage score \n",
    "        t3_per = second_candidates[0:3]\n",
    "        for each in t3_per:\n",
    "            if each not in final_candidates:\n",
    "                final_candidates.insert(len(final_candidates),each)\n",
    "\n",
    "    #rule5: next add any other top results for metrics\n",
    "        othertops = [num_score[0][0],per_score[0][0],tfscore[0][0],order_score[0][0]]\n",
    "        for top in othertops:\n",
    "            if top not in final_candidates:\n",
    "                final_candidates.insert(len(final_candidates),top)\n",
    "                \n",
    "    # unless single term searched, in which case just return \n",
    "    except:\n",
    "        othertops = [num_score[0][0],num_score[1][0],num_score[2][0],per_score[0][0],tfscore[0][0]]\n",
    "        for top in othertops:\n",
    "            if top not in final_candidates:\n",
    "                final_candidates.insert(len(final_candidates),top)\n",
    "\n",
    "    for index, results in enumerate(final_candidates):\n",
    "        if index < 5:\n",
    "            print(\"RESULT\", index + 1, \":\", alldocslist[results][0:100],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "ff1223779f6fe1ff43f57bccb02a34b080fa1bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT 1 : INDONESIA SEES CPO PRICE RISING SHARPLY\n",
      "  Indonesia expects crude palm oil CPO\n",
      "  prices to rise shar ...\n",
      "RESULT 2 : INDONESIAN COMMODITY EXCHANGE MAY EXPAND\n",
      "  The Indonesian Commodity Exchange is\n",
      "  likely to start tr ...\n",
      "RESULT 3 : MALAYSIA MAY NOT MEET 1987 OIL PALM TARGET\n",
      "  Malaysia is unlikely to meet its\n",
      "  targeted output of f ...\n",
      "RESULT 4 : SAUDI ARABIA SEEKING RBD PALM OLEIN\n",
      "  Saudi Arabia is in the market for 4000\n",
      "  tonnes of refined ble ...\n",
      "RESULT 5 : SAUDI ARABIA BUYS RBD PALM OLEIN\n",
      "  Saudi Arabia bought 4000 tonnes of\n",
      "  Malaysian refined bleached d ...\n"
     ]
    }
   ],
   "source": [
    "# example of output \n",
    "rank('indonesia palm oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "e7e54018311df9282fdc70cc31269c03a89ff057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT 1 : CHINA CHILE TO BUILD COPPER TUBE PLANT IN CHINA\n",
      "  Chinas stateowned Beijing NonFerrous\n",
      "  Metals Indu ...\n",
      "RESULT 2 : NIPPON STEEL DENIES CHINA SEEKING JAPANESE PLANTS\n",
      "  Nippon Steel Corp ltNSTCT denied local\n",
      "  newspap ...\n",
      "RESULT 3 : CHINA RAISES GRAIN PURCHASE PRICES\n",
      "  China has raised the state purchase\n",
      "  prices of corn rice cotto ...\n",
      "RESULT 4 : CHINA DAILY SAYS VERMIN EAT 712 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showe ...\n",
      "RESULT 5 : CHINA SULPHURIRON MINE STARTS PRODUCTION\n",
      "  Chinas largest sulphuriron mine has\n",
      "  started trial produ ...\n"
     ]
    }
   ],
   "source": [
    "# example of output \n",
    "rank('china')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "209091e8596be4e04ac1721bd7d700af28b3fd5c"
   },
   "source": [
    "# PART V: Rank and return (machine learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27c241335deb72336169932273b62825312bb5a0"
   },
   "outputs": [],
   "source": [
    "# Create pseudo-truth set using first 5 words \n",
    "# Because I don't have a turth set I will generate a pseudo one by pulling terms from the documents - this is far from perfect\n",
    "     # as it may not approximate well peoples actual queries but it will serve well to build the ML architecture \n",
    "\n",
    "df_truth = pd.DataFrame()\n",
    "\n",
    "for doc in plottest:\n",
    "    first_five = doc[0:5]\n",
    "    test_sentence = ' '.join(first_five)\n",
    "    result = search(test_sentence)\n",
    "    df_temp = pd.DataFrame([result])\n",
    "    df_truth= pd.concat([df_truth, df_temp])\n",
    "\n",
    "df_truth['truth'] = range(0,len(plottest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c430f7b42a7b33e9c0407f6e3e4304a5174d1187"
   },
   "outputs": [],
   "source": [
    "# create another psuedo-truth set using random 3 word sequence from docs\n",
    "\n",
    "df_truth1 = pd.DataFrame()\n",
    "seqlen = 3\n",
    "\n",
    "for doc in plottest:\n",
    "    try:\n",
    "        start = random.randint(0,(len(doc)-seqlen))\n",
    "        random_seq = doc[start:start+seqlen]\n",
    "        test_sentence = ' '.join(random_seq)\n",
    "    except:\n",
    "        test_sentence = doc[0]\n",
    "    result = search(test_sentence)\n",
    "    df_temp = pd.DataFrame([result])\n",
    "    df_truth1= pd.concat([df_truth1, df_temp])\n",
    "\n",
    "df_truth1['truth'] = range(0,len(plottest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32a04b1cf5436be7e889766bdcc7947a6cb5850a"
   },
   "outputs": [],
   "source": [
    "# create another psuedo-truth set using different random 4 word sequence from docs\n",
    "\n",
    "df_truth2 = pd.DataFrame()\n",
    "seqlen = 4\n",
    "\n",
    "for doc in plottest:\n",
    "    try:\n",
    "        start = random.randint(0,(len(doc)-seqlen))\n",
    "        random_seq = doc[start:start+seqlen]\n",
    "        test_sentence = ' '.join(random_seq)\n",
    "    except:\n",
    "        test_sentence = doc[0]\n",
    "    result = search(test_sentence)\n",
    "    df_temp = pd.DataFrame([result])\n",
    "    df_truth2= pd.concat([df_truth2, df_temp])\n",
    "\n",
    "df_truth2['truth'] = range(0,len(plottest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "620159349587576820bf4223f70a5a02cd16f781"
   },
   "outputs": [],
   "source": [
    "# create another psuedo-truth set using different random 2 word sequence from docs\n",
    "\n",
    "df_truth3 = pd.DataFrame()\n",
    "seqlen = 2\n",
    "\n",
    "for doc in plottest:\n",
    "    try:\n",
    "        start = random.randint(0,(len(doc)-seqlen))\n",
    "        random_seq = doc[start:start+seqlen]\n",
    "        test_sentence = ' '.join(random_seq)\n",
    "    except:\n",
    "        test_sentence = doc[0]\n",
    "    result = search(test_sentence)\n",
    "    df_temp = pd.DataFrame([result])\n",
    "    df_truth3= pd.concat([df_truth3, df_temp])\n",
    "\n",
    "df_truth3['truth'] = range(0,len(plottest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d75e90237b4b359091f2aad7b066b77d93984bbe"
   },
   "outputs": [],
   "source": [
    "# combine the truth sets and save to disk \n",
    "truth_set = pd.concat([df_truth,df_truth1,df_truth2,df_truth3])\n",
    "truth_set.columns = ['search term', 'actual_words_searched','num_occur','percentage_of_terms','td-idf','word_order','truth']\n",
    "truth_set.to_csv(\"truth_set_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d6e1c01ee576a5b04cea6578f86fbfe03849d24"
   },
   "outputs": [],
   "source": [
    "truth_set[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d08a5dcd003e8c24a4d6ac3610cf2a84c1b29fe4"
   },
   "outputs": [],
   "source": [
    "truth_set\n",
    "test_set = truth_set[0:3]\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7fffa8821923ff7541d1c874067eb5243b71d900"
   },
   "outputs": [],
   "source": [
    "# convert to long format for ML \n",
    "# WARNING AGAIN THIS IS A SLOW PROCESS DUE TO RAM ILOC - COULD BE OPTIMISED FOR FASTER PERFORMANCE \n",
    "# BUG When min(maxnum, len(truth_set) <- is a int not a list because of very short variable length)\n",
    "\n",
    "# row is row\n",
    "# column is variable\n",
    "# i is the result \n",
    "\n",
    "final_set =  pd.DataFrame()\n",
    "test_set = truth_set[1:100]\n",
    "maxnum = 5\n",
    "\n",
    "for row in range(0,len(test_set.index)):\n",
    "    test_set = truth_set[1:100]\n",
    "    for col in range(2,6):\n",
    "        for i in range(0,min(maxnum,len(truth_set.iloc[row][col]))):\n",
    "            x = pd.DataFrame([truth_set.iloc[row][col][i]])\n",
    "            x['truth'] = truth_set.iloc[row]['truth']\n",
    "            x.columns = [(str(truth_set.columns[col]),\"index\",i),(str(truth_set.columns[col]),\"score\",i),'truth']\n",
    "            test_set = test_set.merge(x,on='truth')\n",
    "    final_set = pd.concat([final_set,test_set])\n",
    "        \n",
    "final_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "595a60b9cc849a8fe7727f969a5d0e4e8bf90a92"
   },
   "outputs": [],
   "source": [
    "final_set.to_csv(\"ML_set_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "709822cde4bb43ad7e53e9a04656f0448c3dc351"
   },
   "outputs": [],
   "source": [
    "final_set2 = final_set.drop(['actual_words_searched','num_occur','percentage_of_terms','search term','td-idf','word_order'], 1)\n",
    "final_set2.to_csv(\"ML_set_100_3.csv\")\n",
    "final_set2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e5bca55011d89717b436b6fd3616bf360d70eec"
   },
   "outputs": [],
   "source": [
    "final_set3 = final_set2\n",
    "final_set3[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a71d140011bc1354cf1dcf089fccd78150f7d2c6"
   },
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as sm\n",
    "import statsmodels.api as sma\n",
    "from statsmodels.tools.eval_measures import mse\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from sklearn import linear_model, feature_selection,preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f65c9fd7a955a88a4273593a1e6b4dd119beaba"
   },
   "outputs": [],
   "source": [
    "final_set3['y'] = final_set3['truth']\n",
    "final_set3 = final_set3.drop(['truth'], 1)\n",
    "final_set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c7f6f6510bd06c32993e4d36a7cf9fea4e7ba90"
   },
   "outputs": [],
   "source": [
    "data = final_set3\n",
    "data.corr()['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df333c592779820bf14adbf2a279e46d3a434f22"
   },
   "outputs": [],
   "source": [
    "data['a'] = data[data.columns[0]]\n",
    "data['b'] = data[data.columns[10]]\n",
    "data['c'] = data[data.columns[20]]\n",
    "data['d'] = data[data.columns[30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32a0a1b5e97377068c7ebd8658b69348846792b5"
   },
   "outputs": [],
   "source": [
    "X = data\n",
    "\n",
    "train,test = train_test_split(X,train_size=0.80)\n",
    "\n",
    "model = sm.ols(formula='y ~ 1 + a + b + c + d', \n",
    "               data=train).fit()\n",
    "\n",
    "modelforout = model \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1fc64479292acecc2ddb741ba0082d98b86126e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sma.graphics.influence_plot(modelforout, ax=ax, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4e6379d1825c51cbad132271c8130fefbd675fc"
   },
   "outputs": [],
   "source": [
    "res = model.resid # residuals\n",
    "fig = sma.qqplot(res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4d0570763722c098545afe97ba78cf36790a66e"
   },
   "source": [
    "# PART VI: FINAL GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57a2884332743196f70be78f346c9941c3287ea4"
   },
   "outputs": [],
   "source": [
    "term = input(\"search: \")\n",
    "rank(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f78e318bcd13eba07f85b49dea78d9baf7f451b7"
   },
   "outputs": [],
   "source": [
    "term = input(\"search: \")\n",
    "try:\n",
    "    result = rank(term)\n",
    "    result\n",
    "    feedback = input(\"were these articles helpful?, (Y/N): \")\n",
    "    if feedback == \"Y\":\n",
    "        np.save('correct_search.npy', worddic) \n",
    "    elif feedback == \"exit\":\n",
    "    else:\n",
    "        print(\"sorry it was not helpful, try again\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1eb819b89e7b58ae7a5cd8469a8ca594db2bb0fb"
   },
   "source": [
    "# TO-DO / Improvements\n",
    "\n",
    "Indexer:\n",
    "- Improve stem/lemm\n",
    "- Add new metrics (e.g. bonus for exact matches / closeness metric)\n",
    "- Add BM25 (and variants)\n",
    "\n",
    "Search Engine:\n",
    "- Add query expansion / synonyms\n",
    "- Add spellchecker functions\n",
    "- Add confidence level \n",
    "\n",
    "Data sources:\n",
    "- Find another source with a proper truth set\n",
    "- Download wikipedia and try it with this \n",
    "\n",
    "Machine Learning:\n",
    "- Fix ML example compiler (crashes if len(col) is so short it is an int and so no len function)\n",
    "- Try different algorithms \n",
    "\n",
    "GUI:\n",
    "- Build GUI interface\n",
    "- Add feedback mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0e20b27db75e6a5d7fce32201a9e7b7b177a624"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
