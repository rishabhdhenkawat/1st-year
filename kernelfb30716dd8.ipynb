{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"markdown","source":"# Inverse indexing, index search, and signal page rankÂ¶"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## PART I: Preparing the documents/webpages"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d299aea0d2f1a76a70043d71b9233b04d985147d"},"cell_type":"code","source":"# Load libraries\n\nimport pandas as pd\nimport numpy as np \nimport string\nimport random\n\nimport nltk\nfrom nltk.corpus import brown\nfrom nltk.corpus import reuters\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import SnowballStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"006be6d61153b59e5f9725e67d268e9f62c78339"},"cell_type":"code","source":"#load 10k reuters news documents \nlen(reuters.fileids())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca9bda17034229327afd7d6cc769425d81bf76e9"},"cell_type":"code","source":"#view text from one document \nreuters.raw(fileids=['test/14826'])[0:201]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adf15a9fb24d881a1a2c4f11a955cd1d75a59106"},"cell_type":"code","source":"# remove punctuation from all DOCs \nexclude = set(string.punctuation)\nalldocslist = []\n\nfor index, i in  enumerate(reuters.fileids()):\n    text = reuters.raw(fileids=[i])\n    text = ''.join(ch for ch in text if ch not in exclude)\n    alldocslist.append(text)\n    \nprint(alldocslist[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bc13232129a1e7c110ac6910451df3c14c91f40"},"cell_type":"code","source":"#tokenize words in all DOCS \nplot_data = [[]] * len(alldocslist)\n\nfor doc in alldocslist:\n    text = doc\n    tokentext = word_tokenize(text)\n    plot_data[index].append(tokentext)\n    \nprint(plot_data[0][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd5c50507cec89313952d57aad07e9d0842ff9e1"},"cell_type":"code","source":"# Navigation: first index gives all documents, second index gives specific document, third index gives words of that doc\nplot_data[0][1][0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f2c197cf782bce0102488515a65ba735f656587"},"cell_type":"code","source":"\n#make all words lower case for all docs \nfor x in range(len(reuters.fileids())):\n    lowers = [word.lower() for word in plot_data[0][x]]\n    plot_data[0][x] = lowers\n\nplot_data[0][1][0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e74f62ed696191d32d14fb815e0e1ab6f8b393b"},"cell_type":"code","source":"# remove stop words from all docs \nstop_words = set(stopwords.words('english'))\n\nfor x in range(len(reuters.fileids())):\n    filtered_sentence = [w for w in plot_data[0][x] if not w in stop_words]\n    plot_data[0][x] = filtered_sentence\n\nplot_data[0][1][0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d47eebfd211946959f31d9606f7f8c20f70f3303"},"cell_type":"code","source":"#stem words EXAMPLE (could try others/lemmers )\n\nsnowball_stemmer = SnowballStemmer(\"english\")\nstemmed_sentence = [snowball_stemmer.stem(w) for w in filtered_sentence]\nstemmed_sentence[0:10]\n\nporter_stemmer = PorterStemmer()\nsnowball_stemmer = SnowballStemmer(\"english\")\nstemmed_sentence = [ porter_stemmer.stem(w) for w in filtered_sentence]\nstemmed_sentence[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db78d4e0b4f69fef8560ad8575d3ea9e17980d7f"},"cell_type":"markdown","source":"# PART II: CREATING THE INVERSE-INDEX"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2a0605e9d7a1dbd52ebfea44ba7ded39fc53767c"},"cell_type":"code","source":"# Create inverse index which gives document number for each document and where word appears\n\n#first we need to create a list of all words \nl = plot_data[0]\nflatten = [item for sublist in l for item in sublist]\nwords = flatten\nwordsunique = set(words)\nwordsunique = list(wordsunique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"49efc97ad7151065564bb516cd3a53ff9297c1f1"},"cell_type":"code","source":"# create functions for TD-IDF / BM25\nimport math\nfrom textblob import TextBlob as tb\n\ndef tf(word, doc):\n    return doc.count(word) / len(doc)\n\ndef n_containing(word, doclist):\n    return sum(1 for doc in doclist if word in doc)\n\ndef idf(word, doclist):\n    return math.log(len(doclist) / (0.01 + n_containing(word, doclist)))\n\ndef tfidf(word, doc, doclist):\n    return (tf(word, doc) * idf(word, doclist))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"52c04e635a0c2519139a48e32f6f9f388bbe99d6"},"cell_type":"code","source":"# Create dictonary of words\n# THIS ONE-TIME INDEXING IS THE MOST PROCESSOR-INTENSIVE STEP AND WILL TAKE TIME TO RUN (BUT ONLY NEEDS TO BE RUN ONCE)\nimport re\nimport numpy as np\n\nplottest = plot_data[0][0:1000]\n\nworddic = {}\n\nfor doc in plottest:\n    for word in wordsunique:\n        if word in doc:\n            word = str(word)\n            index = plottest.index(doc)\n            positions = list(np.where(np.array(plottest[index]) == word)[0])\n            idfs = tfidf(word,doc,plottest)\n            try:\n                worddic[word].append([index,positions,idfs])\n            except:\n                worddic[word] = []\n                worddic[word].append([index,positions,idfs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb59b82d32b95214e512ff468113c12484588498"},"cell_type":"code","source":"# the index creates a dic with each word as a KEY and a list of doc indexs, word positions, and td-idf score as VALUES\nworddic['china']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"812e14b05aaca705a2f5b5b4228cdfb14a71b198"},"cell_type":"code","source":"# pickel (save) the dictonary to avoid re-calculating\nnp.save('worddic_1000.npy', worddic)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bba44192406f7e4118fedc0710d2ac46154a568"},"cell_type":"markdown","source":"# PART III: The Search Engine"},{"metadata":{"trusted":true,"_uuid":"2147470ee4e617ed9bd86dd4cc8ac28a8624cd8f"},"cell_type":"code","source":"# create word search which takes multiple words and finds documents that contain both along with metrics for ranking:\n\n    ## (1) Number of occruances of search words \n    ## (2) TD-IDF score for search words \n    ## (3) Percentage of search terms\n    ## (4) Word ordering score \n    ## (5) Exact match bonus \n\n\nfrom collections import Counter\n\ndef search(searchsentence):\n    try:\n        # split sentence into individual words \n        searchsentence = searchsentence.lower()\n        try:\n            words = searchsentence.split(' ')\n        except:\n            words = list(words)\n        enddic = {}\n        idfdic = {}\n        closedic = {}\n        \n        # remove words if not in worddic \n        realwords = []\n        for word in words:\n            if word in list(worddic.keys()):\n                realwords.append(word)  \n        words = realwords\n        numwords = len(words)\n        \n        # make metric of number of occurances of all words in each doc & largest total IDF \n        for word in words:\n            for indpos in worddic[word]:\n                index = indpos[0]\n                amount = len(indpos[1])\n                idfscore = indpos[2]\n                enddic[index] = amount\n                idfdic[index] = idfscore\n                fullcount_order = sorted(enddic.items(), key=lambda x:x[1], reverse=True)\n                fullidf_order = sorted(idfdic.items(), key=lambda x:x[1], reverse=True)\n\n                \n        # make metric of what percentage of words appear in each doc\n        combo = []\n        alloptions = {k: worddic.get(k, None) for k in (words)}\n        for worddex in list(alloptions.values()):\n            for indexpos in worddex:\n                for indexz in indexpos:\n                    combo.append(indexz)\n        comboindex = combo[::3]\n        combocount = Counter(comboindex)\n        for key in combocount:\n            combocount[key] = combocount[key] / numwords\n        combocount_order = sorted(combocount.items(), key=lambda x:x[1], reverse=True)\n        \n        # make metric for if words appear in same order as in search\n        if len(words) > 1:\n            x = []\n            y = []\n            for record in [worddic[z] for z in words]:\n                for index in record:\n                     x.append(index[0])\n            for i in x:\n                if x.count(i) > 1:\n                    y.append(i)\n            y = list(set(y))\n\n            closedic = {}\n            for wordbig in [worddic[x] for x in words]:\n                for record in wordbig:\n                    if record[0] in y:\n                        index = record[0]\n                        positions = record[1]\n                        try:\n                            closedic[index].append(positions)\n                        except:\n                            closedic[index] = []\n                            closedic[index].append(positions)\n\n            x = 0\n            fdic = {}\n            for index in y:\n                csum = []\n                for seqlist in closedic[index]:\n                    while x > 0:\n                        secondlist = seqlist\n                        x = 0\n                        sol = [1 for i in firstlist if i + 1 in secondlist]\n                        csum.append(sol)\n                        fsum = [item for sublist in csum for item in sublist]\n                        fsum = sum(fsum)\n                        fdic[index] = fsum\n                        fdic_order = sorted(fdic.items(), key=lambda x:x[1], reverse=True)\n                    while x == 0:\n                        firstlist = seqlist\n                        x = x + 1\n        else:\n            fdic_order = 0\n                    \n        # also the one above should be given a big boost if ALL found together \n           \n        \n        #could make another metric for if they are not next to each other but still close \n        \n        \n        return(searchsentence,words,fullcount_order,combocount_order,fullidf_order,fdic_order)\n    \n    except:\n        return(\"\")\n\n\nsearch('indonesia crude palm oil')[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"019fcbd773b5c848507464b3d20c8ce80b5be31f"},"cell_type":"code","source":"# 0 return will give back the search term, the rest will give back metrics (see above)\n\nsearch('indonesia crude palm oil')[1][1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1969ddc3a6ccd58339a4cb22241a92334609d0d2"},"cell_type":"code","source":"# save metrics to dataframe for use in ranking and machine learning \nresult1 = search('china daily says what')\nresult2 = search('indonesia crude palm oil')\nresult3 = search('price of nickel')\nresult4 = search('north yemen sugar')\nresult5 = search('nippon steel')\nresult6 = search('China')\nresult7 = search('Gold')\nresult8 = search('trade')\ndf = pd.DataFrame([result1,result2,result3,result4,result5,result6,result7,result8])\ndf.columns = ['search term', 'actual_words_searched','num_occur','percentage_of_terms','td-idf','word_order']\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d73df8c6ffbff3d6d9f665f85f9b98912a4e8a70"},"cell_type":"code","source":"# look to see if the top documents seem to make sense\n\nalldocslist[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d35e72474705e00e8ac750979812c98aec65e5b4"},"cell_type":"markdown","source":"# PART IV: Rank and return (rules based)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ada71696d2166bee882afe85a2b8fb2b4c10ff2a"},"cell_type":"code","source":"# create a simple (non-machine learning) rank and return function\n\ndef rank(term):\n    results = search(term)\n\n    # get metrics \n    num_score = results[2]\n    per_score = results[3]\n    tfscore = results[4]\n    order_score = results[5]\n    \n    final_candidates = []\n\n    # rule1: if high word order score & 100% percentage terms then put at top position\n    try:\n        first_candidates = []\n\n        for candidates in order_score:\n            if candidates[1] > 1:\n                first_candidates.append(candidates[0])\n\n        second_candidates = []\n\n        for match_candidates in per_score:\n            if match_candidates[1] == 1:\n                second_candidates.append(match_candidates[0])\n            if match_candidates[1] == 1 and match_candidates[0] in first_candidates:\n                final_candidates.append(match_candidates[0])\n\n    # rule2: next add other word order score which are greater than 1 \n\n        t3_order = first_candidates[0:3]\n        for each in t3_order:\n            if each not in final_candidates:\n                final_candidates.insert(len(final_candidates),each)\n\n    # rule3: next add top td-idf results\n        final_candidates.insert(len(final_candidates),tfscore[0][0])\n        final_candidates.insert(len(final_candidates),tfscore[1][0])\n\n    # rule4: next add other high percentage score \n        t3_per = second_candidates[0:3]\n        for each in t3_per:\n            if each not in final_candidates:\n                final_candidates.insert(len(final_candidates),each)\n\n    #rule5: next add any other top results for metrics\n        othertops = [num_score[0][0],per_score[0][0],tfscore[0][0],order_score[0][0]]\n        for top in othertops:\n            if top not in final_candidates:\n                final_candidates.insert(len(final_candidates),top)\n                \n    # unless single term searched, in which case just return \n    except:\n        othertops = [num_score[0][0],num_score[1][0],num_score[2][0],per_score[0][0],tfscore[0][0]]\n        for top in othertops:\n            if top not in final_candidates:\n                final_candidates.insert(len(final_candidates),top)\n\n    for index, results in enumerate(final_candidates):\n        if index < 5:\n            print(\"RESULT\", index + 1, \":\", alldocslist[results][0:100],\"...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff1223779f6fe1ff43f57bccb02a34b080fa1bd3"},"cell_type":"code","source":"# example of output \nrank('indonesia palm oil')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7e54018311df9282fdc70cc31269c03a89ff057"},"cell_type":"code","source":"# example of output \nrank('china')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"209091e8596be4e04ac1721bd7d700af28b3fd5c"},"cell_type":"markdown","source":"# PART V: Rank and return (machine learning)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"27c241335deb72336169932273b62825312bb5a0"},"cell_type":"code","source":"# Create pseudo-truth set using first 5 words \n# Because I don't have a turth set I will generate a pseudo one by pulling terms from the documents - this is far from perfect\n     # as it may not approximate well peoples actual queries but it will serve well to build the ML architecture \n\ndf_truth = pd.DataFrame()\n\nfor doc in plottest:\n    first_five = doc[0:5]\n    test_sentence = ' '.join(first_five)\n    result = search(test_sentence)\n    df_temp = pd.DataFrame([result])\n    df_truth= pd.concat([df_truth, df_temp])\n\ndf_truth['truth'] = range(0,len(plottest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c430f7b42a7b33e9c0407f6e3e4304a5174d1187"},"cell_type":"code","source":"# create another psuedo-truth set using random 3 word sequence from docs\n\ndf_truth1 = pd.DataFrame()\nseqlen = 3\n\nfor doc in plottest:\n    try:\n        start = random.randint(0,(len(doc)-seqlen))\n        random_seq = doc[start:start+seqlen]\n        test_sentence = ' '.join(random_seq)\n    except:\n        test_sentence = doc[0]\n    result = search(test_sentence)\n    df_temp = pd.DataFrame([result])\n    df_truth1= pd.concat([df_truth1, df_temp])\n\ndf_truth1['truth'] = range(0,len(plottest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"32a04b1cf5436be7e889766bdcc7947a6cb5850a"},"cell_type":"code","source":"# create another psuedo-truth set using different random 4 word sequence from docs\n\ndf_truth2 = pd.DataFrame()\nseqlen = 4\n\nfor doc in plottest:\n    try:\n        start = random.randint(0,(len(doc)-seqlen))\n        random_seq = doc[start:start+seqlen]\n        test_sentence = ' '.join(random_seq)\n    except:\n        test_sentence = doc[0]\n    result = search(test_sentence)\n    df_temp = pd.DataFrame([result])\n    df_truth2= pd.concat([df_truth2, df_temp])\n\ndf_truth2['truth'] = range(0,len(plottest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"620159349587576820bf4223f70a5a02cd16f781"},"cell_type":"code","source":"# create another psuedo-truth set using different random 2 word sequence from docs\n\ndf_truth3 = pd.DataFrame()\nseqlen = 2\n\nfor doc in plottest:\n    try:\n        start = random.randint(0,(len(doc)-seqlen))\n        random_seq = doc[start:start+seqlen]\n        test_sentence = ' '.join(random_seq)\n    except:\n        test_sentence = doc[0]\n    result = search(test_sentence)\n    df_temp = pd.DataFrame([result])\n    df_truth3= pd.concat([df_truth3, df_temp])\n\ndf_truth3['truth'] = range(0,len(plottest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d75e90237b4b359091f2aad7b066b77d93984bbe"},"cell_type":"code","source":"# combine the truth sets and save to disk \ntruth_set = pd.concat([df_truth,df_truth1,df_truth2,df_truth3])\ntruth_set.columns = ['search term', 'actual_words_searched','num_occur','percentage_of_terms','td-idf','word_order','truth']\ntruth_set.to_csv(\"truth_set_final.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d6e1c01ee576a5b04cea6578f86fbfe03849d24"},"cell_type":"code","source":"truth_set[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d08a5dcd003e8c24a4d6ac3610cf2a84c1b29fe4"},"cell_type":"code","source":"truth_set\ntest_set = truth_set[0:3]\ntest_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fffa8821923ff7541d1c874067eb5243b71d900"},"cell_type":"code","source":"# convert to long format for ML \n# WARNING AGAIN THIS IS A SLOW PROCESS DUE TO RAM ILOC - COULD BE OPTIMISED FOR FASTER PERFORMANCE \n# BUG When min(maxnum, len(truth_set) <- is a int not a list because of very short variable length)\n\n# row is row\n# column is variable\n# i is the result \n\nfinal_set =  pd.DataFrame()\ntest_set = truth_set[1:100]\nmaxnum = 5\n\nfor row in range(0,len(test_set.index)):\n    test_set = truth_set[1:100]\n    for col in range(2,6):\n        for i in range(0,min(maxnum,len(truth_set.iloc[row][col]))):\n            x = pd.DataFrame([truth_set.iloc[row][col][i]])\n            x['truth'] = truth_set.iloc[row]['truth']\n            x.columns = [(str(truth_set.columns[col]),\"index\",i),(str(truth_set.columns[col]),\"score\",i),'truth']\n            test_set = test_set.merge(x,on='truth')\n    final_set = pd.concat([final_set,test_set])\n        \nfinal_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"595a60b9cc849a8fe7727f969a5d0e4e8bf90a92"},"cell_type":"code","source":"final_set.to_csv(\"ML_set_100.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"709822cde4bb43ad7e53e9a04656f0448c3dc351"},"cell_type":"code","source":"final_set2 = final_set.drop(['actual_words_searched','num_occur','percentage_of_terms','search term','td-idf','word_order'], 1)\nfinal_set2.to_csv(\"ML_set_100_3.csv\")\nfinal_set2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e5bca55011d89717b436b6fd3616bf360d70eec"},"cell_type":"code","source":"final_set3 = final_set2\nfinal_set3[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a71d140011bc1354cf1dcf089fccd78150f7d2c6"},"cell_type":"code","source":"# Load libraries \n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as sm\nimport statsmodels.api as sma\nfrom statsmodels.tools.eval_measures import mse\nfrom statsmodels.tools.tools import add_constant\n\nfrom sklearn import linear_model,cross_validation, feature_selection,preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cross_validation import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f65c9fd7a955a88a4273593a1e6b4dd119beaba"},"cell_type":"code","source":"final_set3['y'] = final_set3['truth']\nfinal_set3 = final_set3.drop(['truth'], 1)\nfinal_set3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c7f6f6510bd06c32993e4d36a7cf9fea4e7ba90"},"cell_type":"code","source":"data = final_set3\ndata.corr()['y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"df333c592779820bf14adbf2a279e46d3a434f22"},"cell_type":"code","source":"data['a'] = data[data.columns[0]]\ndata['b'] = data[data.columns[10]]\ndata['c'] = data[data.columns[20]]\ndata['d'] = data[data.columns[30]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32a0a1b5e97377068c7ebd8658b69348846792b5"},"cell_type":"code","source":"X = data\n\ntrain,test = cross_validation.train_test_split(X,train_size=0.80)\n\nmodel = sm.ols(formula='y ~ 1 + a + b + c + d', \n               data=train).fit()\n\nmodelforout = model \n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1fc64479292acecc2ddb741ba0082d98b86126e"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,8))\nfig = sma.graphics.influence_plot(modelforout, ax=ax, criterion=\"cooks\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4e6379d1825c51cbad132271c8130fefbd675fc"},"cell_type":"code","source":"res = model.resid # residuals\nfig = sma.qqplot(res)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4d0570763722c098545afe97ba78cf36790a66e"},"cell_type":"markdown","source":"# PART VI: FINAL GUI"},{"metadata":{"trusted":true,"_uuid":"57a2884332743196f70be78f346c9941c3287ea4"},"cell_type":"code","source":"term = input(\"search: \")\nrank(term)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f78e318bcd13eba07f85b49dea78d9baf7f451b7"},"cell_type":"code","source":"term = input(\"search: \")\ntry:\n    result = rank(term)\n    result\n    feedback = input(\"were these articles helpful?, (Y/N): \")\n    if feedback == \"Y\":\n        np.save('correct_search.npy', worddic) \n    elif feedback == \"exit\":\n    else:\n        print(\"sorry it was not helpful, try again\")\nexcept:\n    print(\"no results found\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eb819b89e7b58ae7a5cd8469a8ca594db2bb0fb"},"cell_type":"markdown","source":"# TO-DO / Improvements\n\nIndexer:\n- Improve stem/lemm\n- Add new metrics (e.g. bonus for exact matches / closeness metric)\n- Add BM25 (and variants)\n\nSearch Engine:\n- Add query expansion / synonyms\n- Add spellchecker functions\n- Add confidence level \n\nData sources:\n- Find another source with a proper truth set\n- Download wikipedia and try it with this \n\nMachine Learning:\n- Fix ML example compiler (crashes if len(col) is so short it is an int and so no len function)\n- Try different algorithms \n\nGUI:\n- Build GUI interface\n- Add feedback mechanism"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c0e20b27db75e6a5d7fce32201a9e7b7b177a624"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}